# run.sh

#!/bin/bash

# è§£æåƒæ•¸ï¼šé è¨­ç©ºï¼Œå¦‚æœæœ‰ --resume å‚³å…¥å°±ä½¿ç”¨
RESUME_PATH=""

while [[ $# -gt 0 ]]; do
  case $1 in
    --resume)
      RESUME_PATH="$2"
      shift 2
      ;;
    *)
      echo "âŒ ä¸èªè­˜çš„åƒæ•¸: $1"
      exit 1
      ;;
  esac
done

# å–å¾—ç•¶å¤©æ—¥æœŸ
DATE=$(date +"%Y%m%d")

# è‡ªå‹•è¨ˆç®—ç•¶å¤©å¯¦é©—æ¬¡æ•¸
COUNT=$(ls ./EXP 2>/dev/null | grep "run_${DATE}" | wc -l)
COUNT=$(printf "%02d" $((COUNT + 1)))

# è¨­å®š EXP è³‡æ–™å¤¾
EXP_DIR=./EXP/run_${DATE}_${COUNT}
mkdir -p ${EXP_DIR}/TB
mkdir -p ${EXP_DIR}/checkpoint
mkdir -p ${EXP_DIR}/logs

# åŒ¯å‡ºç’°å¢ƒè®Šæ•¸çµ¦ train.py
export EXP_DIR=${EXP_DIR}
export TB_DIR=${EXP_DIR}/TB
export CKPT_DIR=${EXP_DIR}/checkpoint
export RESUME_PATH=${RESUME_PATH}

# TensorBoard è·¯å¾‘è®Šæ•¸
TB_PORT=6006
EXTERNAL_PORT=8812

# è‡ªå‹•åµæ¸¬æœ€æ–° EXP ç›®éŒ„ä¾› TensorBoard ä½¿ç”¨
LATEST_EXP_DIR=$(ls -td ./EXP/run_* | head -n 1)

# å•Ÿå‹• TensorBoard
echo "ğŸš€ å•Ÿå‹• TensorBoard..."
tensorboard --logdir=${LATEST_EXP_DIR} --port=${TB_PORT} --host=0.0.0.0 &

sleep 3

echo "ğŸ“Š TensorBoard å·²å•Ÿå‹•ï¼è«‹åœ¨ç€è¦½å™¨æ‰“é–‹ï¼š"
echo "ğŸ‘‰ http://$(hostname -I | awk '{print $1}'):${EXTERNAL_PORT}"
echo "ğŸ“‚ ç•¶å‰ EXP ç›®éŒ„: ${EXP_DIR}"
echo "ğŸ“‚ TensorBoard ç›®éŒ„: ${LATEST_EXP_DIR}"
echo "ğŸ“‚ Checkpoint ç›®éŒ„: ${CKPT_DIR}"

# é–‹å§‹æ¨¡å‹è¨“ç·´ä¸¦ä¿å­˜çµ‚ç«¯ log
LOG_FILE=${EXP_DIR}/logs/run_${DATE}_${COUNT}.log

if [ -n "$RESUME_PATH" ]; then
  echo "ğŸ”„ æ¢å¾©è¨“ç·´æ¨¡å¼ï¼Œè¼‰å…¥ checkpoint: $RESUME_PATH"
  python -m torch.distributed.launch --nproc_per_node=1 train.py --devices 0 --resume $RESUME_PATH | tee ${LOG_FILE}
else
  python -m torch.distributed.launch --nproc_per_node=1 train.py --devices 0 | tee ${LOG_FILE}
fi

# è¨“ç·´å®Œæˆæç¤ºæœ€æ–° checkpoint
LATEST_CKPT=$(ls -t ${EXP_DIR}/checkpoint/*.pth | head -n 1)
echo ""
echo "âœ… è¨“ç·´å®Œæˆï¼æœ€æ–°çš„ checkpoint å„²å­˜åœ¨ï¼š"
echo "${LATEST_CKPT}"
echo "ğŸš€ ä½ å¯ä»¥ç”¨é€™å€‹è·¯å¾‘ resume è¨“ç·´å“¦ï¼"